{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.colors as mcol\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm.notebook import tqdm_notebook as pbar\n",
    "from sklearn import linear_model\n",
    "from tqdm.notebook import tqdm_notebook as pbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io train\n",
    "X = []\n",
    "Y = []\n",
    "with open(\"synthetic_dataset/train.txt\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        X.append([float(row[0]), float(row[1])])\n",
    "        Y.append([int(row[2])-1])\n",
    "                \n",
    "# PredictorScaler=StandardScaler()\n",
    "# X = PredictorScaler.fit_transform(X)\n",
    "X = np.array(X, dtype=np.float64)\n",
    "Y = np.array(Y, dtype=np.int16)\n",
    "df_X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io dev\n",
    "X_dev = []\n",
    "Y_dev = []\n",
    "with open(\"synthetic_dataset/dev.txt\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        X_dev.append([float(row[0]), float(row[1])])\n",
    "        Y_dev.append([int(row[2])-1])\n",
    "        \n",
    "# X_dev = PredictorScaler.transform(X_dev)\n",
    "X_dev = np.array(X_dev, dtype=np.float64)\n",
    "Y_dev = np.array(Y_dev, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating instance of one-hot-encoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# create df_Y\n",
    "df_Y = pd.DataFrame(np.array(Y,dtype=int))\n",
    "\n",
    "#perform one-hot encoding on  Y\n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(Y.reshape(-1,1)).toarray())\n",
    "Yhat = encoder_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.    , -13.826 ,   4.799 ],\n",
       "       [  1.    , -12.301 ,  -1.3551],\n",
       "       [  1.    , -13.968 ,   4.3138],\n",
       "       ...,\n",
       "       [  1.    ,  -9.4373,   0.1722],\n",
       "       [  1.    ,  -5.1853,   1.0373],\n",
       "       [  1.    ,  -8.9136,  -4.6225]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepend 1 to x to get z\n",
    "Z = []\n",
    "for x in X:\n",
    "    z = [x1 for x1 in x]\n",
    "    z.insert(0,1)\n",
    "    Z.append(z)\n",
    "Z = np.array(Z)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "def dot_sigmoid(x:np.ndarray,y:np.ndarray):\n",
    "    try:\n",
    "        z = np.dot(x,y)\n",
    "    except:\n",
    "        print(x)\n",
    "        print(y)\n",
    "        x = np.array(x).ravel()\n",
    "        z = np.dot(x,y)\n",
    "    return expit(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(y: float, y_pred: float) -> float:\n",
    "    return -((y * np.log(y_pred)))\n",
    "\n",
    "def error(ys: np.ndarray, ys_pred: np.ndarray) -> float:\n",
    "    assert len(ys) == len(ys_pred)\n",
    "    num_items: int = len(ys)\n",
    "    sum_nll: float = np.sum([neg_log_likelihood(y, y_pred) for y, y_pred in zip(ys, ys_pred)])\n",
    "    return (1 / num_items) * sum_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with \"beta\": [0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442c5814b61b4c7992ae19275cce0a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --> loss: 0.5653701772222722\n",
      "Epoch 11 --> loss: 0.4207354355277279\n",
      "Epoch 21 --> loss: 0.40824765119948514\n",
      "Epoch 31 --> loss: 0.3978993301593847\n",
      "Epoch 41 --> loss: 0.38807847924203126\n",
      "Epoch 51 --> loss: 0.3787274182859961\n",
      "Epoch 61 --> loss: 0.3698202961560433\n",
      "Epoch 71 --> loss: 0.36133405368825416\n",
      "Epoch 81 --> loss: 0.35324674815972495\n",
      "Epoch 91 --> loss: 0.3455373778794243\n",
      "Epoch 101 --> loss: 0.3381858665318782\n",
      "Epoch 111 --> loss: 0.3311730646406067\n",
      "Epoch 121 --> loss: 0.3244807463010878\n",
      "Epoch 131 --> loss: 0.3180915983922856\n",
      "Epoch 141 --> loss: 0.3119892029724705\n",
      "Epoch 151 --> loss: 0.3061580141294807\n",
      "Epoch 161 --> loss: 0.3005833305313967\n",
      "Epoch 171 --> loss: 0.2952512647836322\n",
      "Epoch 181 --> loss: 0.29014871054308466\n",
      "Epoch 191 --> loss: 0.28526330819201257\n",
      "Epoch 201 --> loss: 0.2805834097395757\n",
      "Epoch 211 --> loss: 0.2760980434988459\n",
      "Epoch 221 --> loss: 0.27179687898165683\n",
      "Epoch 231 --> loss: 0.2676701923622161\n",
      "Epoch 241 --> loss: 0.263708832782085\n",
      "Epoch 251 --> loss: 0.2599041897026887\n",
      "Epoch 261 --> loss: 0.2562481614557881\n",
      "Epoch 271 --> loss: 0.25273312509607154\n",
      "Epoch 281 --> loss: 0.24935190762200019\n",
      "Epoch 291 --> loss: 0.2460977586001498\n",
      "Epoch 301 --> loss: 0.24296432420353592\n",
      "Epoch 311 --> loss: 0.23994562265480973\n",
      "Epoch 321 --> loss: 0.23703602104997681\n",
      "Epoch 331 --> loss: 0.23423021352673018\n",
      "Epoch 341 --> loss: 0.23152320073288807\n",
      "Epoch 351 --> loss: 0.22891027054433832\n",
      "Epoch 361 --> loss: 0.22638697997780702\n",
      "Epoch 371 --> loss: 0.2239491382412901\n",
      "Epoch 381 --> loss: 0.2215927908638334\n",
      "Epoch 391 --> loss: 0.2193142048461754\n",
      "Epoch 401 --> loss: 0.21710985477442593\n",
      "Epoch 411 --> loss: 0.21497640984018318\n",
      "Epoch 421 --> loss: 0.212910721712201\n",
      "Epoch 431 --> loss: 0.21090981320671803\n",
      "Epoch 441 --> loss: 0.20897086770581588\n",
      "Epoch 451 --> loss: 0.2070912192755475\n",
      "Epoch 461 --> loss: 0.20526834343803504\n",
      "Epoch 471 --> loss: 0.2034998485542221\n",
      "Epoch 481 --> loss: 0.20178346777643077\n",
      "Epoch 491 --> loss: 0.2001170515322992\n",
      "Epoch 501 --> loss: 0.1984985605040437\n",
      "Epoch 511 --> loss: 0.1969260590692465\n",
      "Epoch 521 --> loss: 0.19539770917157742\n",
      "Epoch 531 --> loss: 0.19391176459190984\n",
      "Epoch 541 --> loss: 0.19246656559230108\n",
      "Epoch 551 --> loss: 0.19106053390713834\n",
      "Epoch 561 --> loss: 0.1896921680575459\n",
      "Epoch 571 --> loss: 0.18836003896677742\n",
      "Epoch 581 --> loss: 0.1870627858558941\n",
      "Epoch 591 --> loss: 0.18579911240046112\n",
      "Epoch 601 --> loss: 0.18456778313037\n",
      "Epoch 611 --> loss: 0.18336762005614543\n",
      "Epoch 621 --> loss: 0.18219749950629593\n",
      "Epoch 631 --> loss: 0.18105634916134442\n",
      "Epoch 641 --> loss: 0.17994314527120997\n",
      "Epoch 651 --> loss: 0.1788569100435671\n",
      "Epoch 661 --> loss: 0.1777967091916708\n",
      "Epoch 671 --> loss: 0.17676164963097324\n",
      "Epoch 681 --> loss: 0.17575087731461328\n",
      "Epoch 691 --> loss: 0.1747635751985529\n",
      "Epoch 701 --> loss: 0.17379896132780773\n",
      "Epoch 711 --> loss: 0.17285628703579978\n",
      "Epoch 721 --> loss: 0.17193483524944897\n",
      "Epoch 731 --> loss: 0.17103391889311578\n",
      "Epoch 741 --> loss: 0.1701528793850021\n",
      "Epoch 751 --> loss: 0.1692910852200636\n",
      "Epoch 761 --> loss: 0.16844793063389515\n",
      "Epoch 771 --> loss: 0.1676228343424404\n",
      "Epoch 781 --> loss: 0.16681523835272444\n",
      "Epoch 791 --> loss: 0.16602460684015014\n",
      "Epoch 801 --> loss: 0.1652504250881913\n",
      "Epoch 811 --> loss: 0.16449219848660607\n",
      "Epoch 821 --> loss: 0.16374945158455712\n",
      "Epoch 831 --> loss: 0.16302172719526836\n",
      "Epoch 841 --> loss: 0.1623085855490685\n",
      "Epoch 851 --> loss: 0.16160960349189338\n",
      "Epoch 861 --> loss: 0.16092437372650198\n",
      "Epoch 871 --> loss: 0.16025250409385128\n",
      "Epoch 881 --> loss: 0.1595936168922358\n",
      "Epoch 891 --> loss: 0.1589473482319626\n",
      "Epoch 901 --> loss: 0.15831334742346698\n",
      "Epoch 911 --> loss: 0.157691276396923\n",
      "Epoch 921 --> loss: 0.15708080915151168\n",
      "Epoch 931 --> loss: 0.15648163123264455\n",
      "Epoch 941 --> loss: 0.15589343923553695\n",
      "Epoch 951 --> loss: 0.15531594033362817\n",
      "Epoch 961 --> loss: 0.15474885183044534\n",
      "Epoch 971 --> loss: 0.15419190073359307\n",
      "Epoch 981 --> loss: 0.15364482334963114\n",
      "Epoch 991 --> loss: 0.1531073648986794\n",
      "Epoch 1001 --> loss: 0.1525792791476637\n",
      "Epoch 1011 --> loss: 0.15206032806118097\n",
      "Epoch 1021 --> loss: 0.15155028146902147\n",
      "Epoch 1031 --> loss: 0.15104891674944793\n",
      "Epoch 1041 --> loss: 0.15055601852738454\n",
      "Epoch 1051 --> loss: 0.15007137838671725\n",
      "Epoch 1061 --> loss: 0.1495947945959573\n",
      "Epoch 1071 --> loss: 0.1491260718465597\n",
      "Epoch 1081 --> loss: 0.14866502100324047\n",
      "Epoch 1091 --> loss: 0.1482114588656539\n",
      "Epoch 1101 --> loss: 0.14776520794085685\n",
      "Epoch 1111 --> loss: 0.1473260962259903\n",
      "Epoch 1121 --> loss: 0.14689395700066812\n",
      "Epoch 1131 --> loss: 0.14646862862856874\n",
      "Epoch 1141 --> loss: 0.14604995436777204\n",
      "Epoch 1151 --> loss: 0.14563778218939846\n",
      "Epoch 1161 --> loss: 0.14523196460413765\n",
      "Epoch 1171 --> loss: 0.144832358496274\n",
      "Epoch 1181 --> loss: 0.14443882496484006\n",
      "Epoch 1191 --> loss: 0.14405122917154892\n",
      "Epoch 1201 --> loss: 0.14366944019517375\n",
      "Epoch 1211 --> loss: 0.14329333089206434\n",
      "Epoch 1221 --> loss: 0.14292277776250423\n",
      "Epoch 1231 --> loss: 0.1425576608226272\n",
      "Epoch 1241 --> loss: 0.14219786348163385\n",
      "Epoch 1251 --> loss: 0.14184327242405054\n",
      "Epoch 1261 --> loss: 0.14149377749680075\n",
      "Epoch 1271 --> loss: 0.14114927160085905\n",
      "Epoch 1281 --> loss: 0.14080965058727749\n",
      "Epoch 1291 --> loss: 0.1404748131573806\n",
      "Epoch 1301 --> loss: 0.1401446607669399\n",
      "Epoch 1311 --> loss: 0.13981909753414407\n",
      "Epoch 1321 --> loss: 0.1394980301511944\n",
      "Epoch 1331 --> loss: 0.13918136779936152\n",
      "Epoch 1341 --> loss: 0.13886902206734544\n",
      "Epoch 1351 --> loss: 0.13856090687279735\n",
      "Epoch 1361 --> loss: 0.13825693838685688\n",
      "Epoch 1371 --> loss: 0.137957034961576\n",
      "Epoch 1381 --> loss: 0.13766111706010106\n",
      "Epoch 1391 --> loss: 0.13736910718949372\n",
      "Epoch 1401 --> loss: 0.13708092983607473\n",
      "Epoch 1411 --> loss: 0.1367965114031845\n",
      "Epoch 1421 --> loss: 0.1365157801512553\n",
      "Epoch 1431 --> loss: 0.13623866614009425\n",
      "Epoch 1441 --> loss: 0.1359651011732889\n",
      "Epoch 1451 --> loss: 0.13569501874464077\n",
      "Epoch 1461 --> loss: 0.13542835398654465\n",
      "Epoch 1471 --> loss: 0.13516504362023038\n",
      "Epoch 1481 --> loss: 0.1349050259077927\n",
      "Epoch 1491 --> loss: 0.1346482406059333\n",
      "Epoch 1501 --> loss: 0.13439462892134466\n",
      "Epoch 1511 --> loss: 0.1341441334676698\n",
      "Epoch 1521 --> loss: 0.1338966982239745\n",
      "Epoch 1531 --> loss: 0.13365226849466816\n",
      "Epoch 1541 --> loss: 0.1334107908708178\n",
      "Epoch 1551 --> loss: 0.13317221319280004\n",
      "Epoch 1561 --> loss: 0.13293648451423334\n",
      "Epoch 1571 --> loss: 0.13270355506714623\n",
      "Epoch 1581 --> loss: 0.1324733762283277\n",
      "Epoch 1591 --> loss: 0.13224590048681684\n",
      "Epoch 1601 --> loss: 0.1320210814124847\n",
      "Epoch 1611 --> loss: 0.13179887362566936\n",
      "Epoch 1621 --> loss: 0.13157923276782013\n",
      "Epoch 1631 --> loss: 0.1313621154731137\n",
      "Epoch 1641 --> loss: 0.13114747934100782\n",
      "Epoch 1651 --> loss: 0.1309352829096909\n",
      "Epoch 1661 --> loss: 0.13072548563039973\n",
      "Epoch 1671 --> loss: 0.13051804784256973\n",
      "Epoch 1681 --> loss: 0.13031293074978645\n",
      "Epoch 1691 --> loss: 0.13011009639650953\n",
      "Epoch 1701 --> loss: 0.1299095076455408\n",
      "Epoch 1711 --> loss: 0.12971112815620733\n",
      "Epoch 1721 --> loss: 0.1295149223632362\n",
      "Epoch 1731 --> loss: 0.1293208554562917\n",
      "Epoch 1741 --> loss: 0.1291288933601556\n",
      "Epoch 1751 --> loss: 0.12893900271552308\n",
      "Epoch 1761 --> loss: 0.12875115086039474\n",
      "Epoch 1771 --> loss: 0.1285653058120433\n",
      "Epoch 1781 --> loss: 0.1283814362495332\n",
      "Epoch 1791 --> loss: 0.12819951149677317\n",
      "Epoch 1801 --> loss: 0.1280195015060869\n",
      "Epoch 1811 --> loss: 0.12784137684227834\n",
      "Epoch 1821 --> loss: 0.12766510866717706\n",
      "Epoch 1831 --> loss: 0.12749066872464837\n",
      "Epoch 1841 --> loss: 0.12731802932604794\n",
      "Epoch 1851 --> loss: 0.1271471633361112\n",
      "Epoch 1861 --> loss: 0.1269780441592569\n",
      "Epoch 1871 --> loss: 0.12681064572629458\n",
      "Epoch 1881 --> loss: 0.12664494248152144\n",
      "Epoch 1891 --> loss: 0.12648090937019446\n",
      "Epoch 1901 --> loss: 0.12631852182636558\n",
      "Epoch 1911 --> loss: 0.12615775576106844\n",
      "Epoch 1921 --> loss: 0.12599858755084448\n",
      "Epoch 1931 --> loss: 0.12584099402659682\n",
      "Epoch 1941 --> loss: 0.12568495246276187\n",
      "Epoch 1951 --> loss: 0.1255304405667867\n",
      "Epoch 1961 --> loss: 0.12537743646890517\n",
      "Epoch 1971 --> loss: 0.1252259187121987\n",
      "Epoch 1981 --> loss: 0.1250758662429374\n",
      "Epoch 1991 --> loss: 0.1249272584011875\n",
      "Epoch 2001 --> loss: 0.1247800749116812\n",
      "Epoch 2011 --> loss: 0.12463429587493702\n",
      "Epoch 2021 --> loss: 0.12448990175862423\n",
      "Epoch 2031 --> loss: 0.12434687338916359\n",
      "Epoch 2041 --> loss: 0.12420519194355646\n",
      "Epoch 2051 --> loss: 0.12406483894143583\n",
      "Epoch 2061 --> loss: 0.12392579623733105\n",
      "Epoch 2071 --> loss: 0.1237880460131411\n",
      "Epoch 2081 --> loss: 0.12365157077080831\n",
      "Epoch 2091 --> loss: 0.12351635332518852\n",
      "Epoch 2101 --> loss: 0.12338237679710913\n",
      "Epoch 2111 --> loss: 0.12324962460661096\n",
      "Epoch 2121 --> loss: 0.12311808046636762\n",
      "Epoch 2131 --> loss: 0.12298772837527759\n",
      "Epoch 2141 --> loss: 0.12285855261222253\n",
      "Epoch 2151 --> loss: 0.12273053772998943\n",
      "Epoch 2161 --> loss: 0.12260366854934665\n",
      "Epoch 2171 --> loss: 0.12247793015327675\n",
      "Epoch 2181 --> loss: 0.12235330788135293\n",
      "Epoch 2191 --> loss: 0.1222297873242627\n",
      "Epoch 2201 --> loss: 0.12210735431846793\n",
      "Epoch 2211 --> loss: 0.12198599494100165\n",
      "Epoch 2221 --> loss: 0.12186569550439635\n",
      "Epoch 2231 --> loss: 0.12174644255173867\n",
      "Epoch 2241 --> loss: 0.12162822285184786\n",
      "Epoch 2251 --> loss: 0.12151102339457522\n",
      "Epoch 2261 --> loss: 0.12139483138621963\n",
      "Epoch 2271 --> loss: 0.12127963424505615\n",
      "Epoch 2281 --> loss: 0.12116541959697513\n",
      "Epoch 2291 --> loss: 0.12105217527122841\n",
      "Epoch 2301 --> loss: 0.12093988929627804\n",
      "Epoch 2311 --> loss: 0.12082854989574801\n",
      "Epoch 2321 --> loss: 0.12071814548447242\n",
      "Epoch 2331 --> loss: 0.1206086646646399\n",
      "Epoch 2341 --> loss: 0.12050009622203052\n",
      "Epoch 2351 --> loss: 0.12039242912234237\n",
      "Epoch 2361 --> loss: 0.12028565250760571\n",
      "Epoch 2371 --> loss: 0.1201797556926842\n",
      "Epoch 2381 --> loss: 0.12007472816185526\n",
      "Epoch 2391 --> loss: 0.11997055956547541\n",
      "Epoch 2401 --> loss: 0.11986723971671978\n",
      "Epoch 2411 --> loss: 0.11976475858840155\n",
      "Epoch 2421 --> loss: 0.11966310630986204\n",
      "Epoch 2431 --> loss: 0.11956227316393665\n",
      "Epoch 2441 --> loss: 0.11946224958398771\n",
      "Epoch 2451 --> loss: 0.11936302615100752\n",
      "Epoch 2461 --> loss: 0.11926459359078755\n",
      "Epoch 2471 --> loss: 0.11916694277115217\n",
      "Epoch 2481 --> loss: 0.11907006469925596\n",
      "Epoch 2491 --> loss: 0.11897395051894123\n",
      "Epoch 2501 --> loss: 0.1188785915081569\n",
      "Epoch 2511 --> loss: 0.11878397907643376\n",
      "Epoch 2521 --> loss: 0.11869010476241802\n",
      "Epoch 2531 --> loss: 0.11859696023145933\n",
      "Epoch 2541 --> loss: 0.11850453727325198\n",
      "Epoch 2551 --> loss: 0.11841282779952968\n",
      "Epoch 2561 --> loss: 0.1183218238418103\n",
      "Epoch 2571 --> loss: 0.11823151754919155\n",
      "Epoch 2581 --> loss: 0.11814190118619294\n",
      "Epoch 2591 --> loss: 0.11805296713064815\n",
      "Epoch 2601 --> loss: 0.11796470787163936\n",
      "Epoch 2611 --> loss: 0.11787711600748087\n",
      "Epoch 2621 --> loss: 0.11779018424374267\n",
      "Epoch 2631 --> loss: 0.11770390539131886\n",
      "Epoch 2641 --> loss: 0.11761827236453629\n",
      "Epoch 2651 --> loss: 0.11753327817930545\n",
      "Epoch 2661 --> loss: 0.11744891595130874\n",
      "Epoch 2671 --> loss: 0.1173651788942283\n",
      "Epoch 2681 --> loss: 0.1172820603180124\n",
      "Epoch 2691 --> loss: 0.1171995536271762\n",
      "Epoch 2701 --> loss: 0.11711765231914043\n",
      "Epoch 2711 --> loss: 0.11703634998260355\n",
      "Epoch 2721 --> loss: 0.11695564029594907\n",
      "Epoch 2731 --> loss: 0.11687551702568444\n",
      "Epoch 2741 --> loss: 0.11679597402491482\n",
      "Epoch 2751 --> loss: 0.11671700523184557\n",
      "Epoch 2761 --> loss: 0.11663860466831753\n",
      "Epoch 2771 --> loss: 0.11656076643837242\n",
      "Epoch 2781 --> loss: 0.11648348472684668\n",
      "Epoch 2791 --> loss: 0.11640675379799464\n",
      "Epoch 2801 --> loss: 0.11633056799413935\n",
      "Epoch 2811 --> loss: 0.11625492173435148\n",
      "Epoch 2821 --> loss: 0.11617980951315397\n",
      "Epoch 2831 --> loss: 0.11610522589925314\n",
      "Epoch 2841 --> loss: 0.11603116553429636\n",
      "Epoch 2851 --> loss: 0.1159576231316522\n",
      "Epoch 2861 --> loss: 0.11588459347521762\n",
      "Epoch 2871 --> loss: 0.11581207141824702\n",
      "Epoch 2881 --> loss: 0.11574005188220497\n",
      "Epoch 2891 --> loss: 0.11566852985564192\n",
      "Epoch 2901 --> loss: 0.11559750039309177\n",
      "Epoch 2911 --> loss: 0.11552695861399163\n",
      "Epoch 2921 --> loss: 0.11545689970162126\n",
      "Epoch 2931 --> loss: 0.11538731890206491\n",
      "Epoch 2941 --> loss: 0.1153182115231929\n",
      "Epoch 2951 --> loss: 0.1152495729336616\n",
      "Epoch 2961 --> loss: 0.11518139856193475\n",
      "Epoch 2971 --> loss: 0.11511368389532224\n",
      "Epoch 2981 --> loss: 0.11504642447903735\n",
      "Epoch 2991 --> loss: 0.11497961591527345\n",
      "Epoch 3001 --> loss: 0.11491325386229656\n",
      "Epoch 3011 --> loss: 0.11484733403355572\n",
      "Epoch 3021 --> loss: 0.11478185219681093\n",
      "Epoch 3031 --> loss: 0.11471680417327684\n",
      "Epoch 3041 --> loss: 0.11465218583678276\n",
      "Epoch 3051 --> loss: 0.1145879931129481\n",
      "Epoch 3061 --> loss: 0.11452422197837396\n",
      "Epoch 3071 --> loss: 0.11446086845984997\n",
      "Epoch 3081 --> loss: 0.11439792863357416\n",
      "Epoch 3091 --> loss: 0.11433539862438884\n",
      "Epoch 3101 --> loss: 0.11427327460503126\n",
      "Epoch 3111 --> loss: 0.1142115527953959\n",
      "Epoch 3121 --> loss: 0.11415022946181205\n",
      "Epoch 3131 --> loss: 0.11408930091633415\n",
      "Epoch 3141 --> loss: 0.114028763516044\n",
      "Epoch 3151 --> loss: 0.11396861366236906\n",
      "Epoch 3161 --> loss: 0.11390884780040775\n",
      "Epoch 3171 --> loss: 0.11384946241827265\n",
      "Epoch 3181 --> loss: 0.11379045404644096\n",
      "Epoch 3191 --> loss: 0.11373181925712016\n",
      "Epoch 3201 --> loss: 0.11367355466362174\n",
      "Epoch 3211 --> loss: 0.11361565691974988\n",
      "Epoch 3221 --> loss: 0.11355812271919706\n",
      "Epoch 3231 --> loss: 0.11350094879495377\n",
      "Epoch 3241 --> loss: 0.11344413191872682\n",
      "Epoch 3251 --> loss: 0.11338766890036837\n",
      "Epoch 3261 --> loss: 0.11333155658731485\n",
      "Epoch 3271 --> loss: 0.11327579186403684\n",
      "Epoch 3281 --> loss: 0.11322037165149655\n",
      "Epoch 3291 --> loss: 0.1131652929066167\n",
      "Epoch 3301 --> loss: 0.11311055262175787\n",
      "Epoch 3311 --> loss: 0.11305614782420435\n",
      "Epoch 3321 --> loss: 0.11300207557566039\n",
      "Epoch 3331 --> loss: 0.11294833297175415\n",
      "Epoch 3341 --> loss: 0.11289491714154902\n",
      "Epoch 3351 --> loss: 0.11284182524706644\n",
      "Epoch 3361 --> loss: 0.1127890544828143\n",
      "Epoch 3371 --> loss: 0.11273660207532349\n",
      "Epoch 3381 --> loss: 0.11268446528269445\n",
      "Epoch 3391 --> loss: 0.1126326413941489\n",
      "Epoch 3401 --> loss: 0.11258112772959067\n",
      "Epoch 3411 --> loss: 0.11252992163917375\n",
      "Epoch 3421 --> loss: 0.11247902050287671\n",
      "Epoch 3431 --> loss: 0.1124284217300862\n",
      "Epoch 3441 --> loss: 0.11237812275918478\n",
      "Epoch 3451 --> loss: 0.11232812105714803\n",
      "Epoch 3461 --> loss: 0.11227841411914696\n",
      "Epoch 3471 --> loss: 0.11222899946815776\n",
      "Epoch 3481 --> loss: 0.11217987465457722\n",
      "Epoch 3491 --> loss: 0.11213103725584568\n",
      "Epoch 3501 --> loss: 0.11208248487607493\n",
      "Epoch 3511 --> loss: 0.11203421514568322\n",
      "Epoch 3521 --> loss: 0.11198622572103564\n",
      "Epoch 3531 --> loss: 0.11193851428409007\n",
      "Epoch 3541 --> loss: 0.11189107854205013\n",
      "Epoch 3551 --> loss: 0.11184391622702315\n",
      "Epoch 3561 --> loss: 0.11179702509568293\n",
      "Epoch 3571 --> loss: 0.11175040292893883\n",
      "Epoch 3581 --> loss: 0.1117040475316102\n",
      "Epoch 3591 --> loss: 0.11165795673210532\n",
      "Epoch 3601 --> loss: 0.11161212838210663\n",
      "Epoch 3611 --> loss: 0.11156656035625975\n",
      "Epoch 3621 --> loss: 0.11152125055186816\n",
      "Epoch 3631 --> loss: 0.111476196888593\n",
      "Epoch 3641 --> loss: 0.11143139730815715\n",
      "Epoch 3651 --> loss: 0.11138684977405347\n",
      "Epoch 3661 --> loss: 0.11134255227125882\n",
      "Epoch 3671 --> loss: 0.11129850280595266\n",
      "Epoch 3681 --> loss: 0.11125469940523793\n",
      "Epoch 3691 --> loss: 0.11121114011686921\n",
      "Epoch 3701 --> loss: 0.11116782300898272\n",
      "Epoch 3711 --> loss: 0.11112474616983276\n",
      "Epoch 3721 --> loss: 0.11108190770752913\n",
      "Epoch 3731 --> loss: 0.11103930574978305\n",
      "Epoch 3741 --> loss: 0.11099693844365216\n",
      "Epoch 3751 --> loss: 0.11095480395529289\n",
      "Epoch 3761 --> loss: 0.11091290046971565\n",
      "Epoch 3771 --> loss: 0.11087122619054292\n",
      "Epoch 3781 --> loss: 0.1108297793397725\n",
      "Epoch 3791 --> loss: 0.11078855815754303\n",
      "Epoch 3801 --> loss: 0.11074756090190452\n",
      "Epoch 3811 --> loss: 0.11070678584859031\n",
      "Epoch 3821 --> loss: 0.11066623129079503\n",
      "Epoch 3831 --> loss: 0.11062589553895355\n",
      "Epoch 3841 --> loss: 0.11058577692052482\n",
      "Epoch 3851 --> loss: 0.11054587377977865\n",
      "Epoch 3861 --> loss: 0.1105061844775848\n",
      "Epoch 3871 --> loss: 0.11046670739120652\n",
      "Epoch 3881 --> loss: 0.11042744091409655\n",
      "Epoch 3891 --> loss: 0.11038838345569547\n",
      "Epoch 3901 --> loss: 0.11034953344123472\n",
      "Epoch 3911 --> loss: 0.1103108893115407\n",
      "Epoch 3921 --> loss: 0.11027244952284362\n",
      "Epoch 3931 --> loss: 0.1102342125465869\n",
      "Epoch 3941 --> loss: 0.11019617686924167\n",
      "Epoch 3951 --> loss: 0.11015834099212271\n",
      "Epoch 3961 --> loss: 0.1101207034312069\n",
      "Epoch 3971 --> loss: 0.11008326271695568\n",
      "Epoch 3981 --> loss: 0.1100460173941382\n",
      "Epoch 3991 --> loss: 0.1100089660216584\n",
      "Epoch 4001 --> loss: 0.10997210717238463\n",
      "Epoch 4011 --> loss: 0.10993543943298058\n",
      "Epoch 4021 --> loss: 0.10989896140374017\n",
      "Epoch 4031 --> loss: 0.10986267169842302\n",
      "Epoch 4041 --> loss: 0.1098265689440946\n",
      "Epoch 4051 --> loss: 0.10979065178096582\n",
      "Epoch 4061 --> loss: 0.10975491886223807\n",
      "Epoch 4071 --> loss: 0.10971936885394776\n",
      "Epoch 4081 --> loss: 0.1096840004348145\n",
      "Epoch 4091 --> loss: 0.1096488122960908\n",
      "Epoch 4101 --> loss: 0.10961380314141489\n",
      "Epoch 4111 --> loss: 0.10957897168666442\n",
      "Epoch 4121 --> loss: 0.10954431665981244\n",
      "Epoch 4131 --> loss: 0.10950983680078662\n",
      "Epoch 4141 --> loss: 0.10947553086132847\n",
      "Epoch 4151 --> loss: 0.10944139760485591\n",
      "Epoch 4161 --> loss: 0.10940743580632839\n",
      "Epoch 4171 --> loss: 0.10937364425211149\n",
      "Epoch 4181 --> loss: 0.10934002173984514\n",
      "Epoch 4191 --> loss: 0.1093065670783139\n",
      "Epoch 4201 --> loss: 0.10927327908731861\n",
      "Epoch 4211 --> loss: 0.1092401565975486\n",
      "Epoch 4221 --> loss: 0.10920719845045787\n",
      "Epoch 4231 --> loss: 0.10917440349814128\n",
      "Epoch 4241 --> loss: 0.1091417706032131\n",
      "Epoch 4251 --> loss: 0.10910929863868717\n",
      "Epoch 4261 --> loss: 0.10907698648785868\n",
      "Epoch 4271 --> loss: 0.10904483304418733\n",
      "Epoch 4281 --> loss: 0.1090128372111828\n",
      "Epoch 4291 --> loss: 0.10898099790229021\n",
      "Epoch 4301 --> loss: 0.10894931404077939\n",
      "Epoch 4311 --> loss: 0.10891778455963345\n",
      "Epoch 4321 --> loss: 0.1088864084014403\n",
      "Epoch 4331 --> loss: 0.10885518451828528\n",
      "Epoch 4341 --> loss: 0.10882411187164426\n",
      "Epoch 4351 --> loss: 0.1087931894322803\n",
      "Epoch 4361 --> loss: 0.1087624161801393\n",
      "Epoch 4371 --> loss: 0.10873179110424813\n",
      "Epoch 4381 --> loss: 0.10870131320261517\n",
      "Epoch 4391 --> loss: 0.10867098148212988\n",
      "Epoch 4401 --> loss: 0.10864079495846564\n",
      "Epoch 4411 --> loss: 0.10861075265598273\n",
      "Epoch 4421 --> loss: 0.10858085360763357\n",
      "Epoch 4431 --> loss: 0.10855109685486809\n",
      "Epoch 4441 --> loss: 0.10852148144754083\n",
      "Epoch 4451 --> loss: 0.10849200644382022\n",
      "Epoch 4461 --> loss: 0.10846267091009705\n",
      "Epoch 4471 --> loss: 0.10843347392089572\n",
      "Epoch 4481 --> loss: 0.10840441455878617\n",
      "Epoch 4491 --> loss: 0.10837549191429703\n",
      "Epoch 4501 --> loss: 0.108346705085829\n",
      "Epoch 4511 --> loss: 0.1083180531795719\n",
      "Epoch 4521 --> loss: 0.10828953530941879\n",
      "Epoch 4531 --> loss: 0.10826115059688474\n",
      "Epoch 4541 --> loss: 0.10823289817102573\n",
      "Epoch 4551 --> loss: 0.108204777168357\n",
      "Epoch 4561 --> loss: 0.10817678673277442\n",
      "Epoch 4571 --> loss: 0.10814892601547561\n",
      "Epoch 4581 --> loss: 0.10812119417488336\n",
      "Epoch 4591 --> loss: 0.10809359037656774\n",
      "Epoch 4601 --> loss: 0.1080661137931726\n",
      "Epoch 4611 --> loss: 0.10803876360433913\n",
      "Epoch 4621 --> loss: 0.10801153899663314\n",
      "Epoch 4631 --> loss: 0.1079844391634729\n",
      "Epoch 4641 --> loss: 0.10795746330505651\n",
      "Epoch 4651 --> loss: 0.10793061062829147\n",
      "Epoch 4661 --> loss: 0.10790388034672516\n",
      "Epoch 4671 --> loss: 0.10787727168047441\n",
      "Epoch 4681 --> loss: 0.10785078385615902\n",
      "Epoch 4691 --> loss: 0.10782441610683327\n",
      "Epoch 4701 --> loss: 0.10779816767191945\n",
      "Epoch 4711 --> loss: 0.10777203779714283\n",
      "Epoch 4721 --> loss: 0.10774602573446573\n",
      "Epoch 4731 --> loss: 0.10772013074202455\n",
      "Epoch 4741 --> loss: 0.1076943520840653\n",
      "Epoch 4751 --> loss: 0.10766868903088225\n",
      "Epoch 4761 --> loss: 0.1076431408587549\n",
      "Epoch 4771 --> loss: 0.1076177068498883\n",
      "Epoch 4781 --> loss: 0.1075923862923519\n",
      "Epoch 4791 --> loss: 0.10756717848001966\n",
      "Epoch 4801 --> loss: 0.10754208271251285\n",
      "Epoch 4811 --> loss: 0.10751709829513982\n",
      "Epoch 4821 --> loss: 0.10749222453884029\n",
      "Epoch 4831 --> loss: 0.10746746076012802\n",
      "Epoch 4841 --> loss: 0.1074428062810348\n",
      "Epoch 4851 --> loss: 0.10741826042905478\n",
      "Epoch 4861 --> loss: 0.10739382253709037\n",
      "Epoch 4871 --> loss: 0.10736949194339805\n",
      "Epoch 4881 --> loss: 0.10734526799153478\n",
      "Epoch 4891 --> loss: 0.10732115003030519\n",
      "Epoch 4901 --> loss: 0.10729713741371018\n",
      "Epoch 4911 --> loss: 0.10727322950089412\n",
      "Epoch 4921 --> loss: 0.10724942565609523\n",
      "Epoch 4931 --> loss: 0.1072257252485949\n",
      "Epoch 4941 --> loss: 0.10720212765266743\n",
      "Epoch 4951 --> loss: 0.10717863224753174\n",
      "Epoch 4961 --> loss: 0.10715523841730265\n",
      "Epoch 4971 --> loss: 0.10713194555094215\n",
      "Epoch 4981 --> loss: 0.10710875304221314\n",
      "Epoch 4991 --> loss: 0.10708566028963198\n",
      "Epoch 5001 --> loss: 0.10706266669642217\n",
      "Epoch 5011 --> loss: 0.10703977167046902\n",
      "Epoch 5021 --> loss: 0.10701697462427354\n",
      "Epoch 5031 --> loss: 0.10699427497490883\n",
      "Epoch 5041 --> loss: 0.10697167214397538\n",
      "Epoch 5051 --> loss: 0.10694916555755693\n",
      "Epoch 5061 --> loss: 0.10692675464617869\n",
      "Epoch 5071 --> loss: 0.10690443884476268\n",
      "Epoch 5081 --> loss: 0.10688221759258781\n",
      "Epoch 5091 --> loss: 0.10686009033324595\n",
      "Epoch 5101 --> loss: 0.10683805651460257\n",
      "Epoch 5111 --> loss: 0.10681611558875498\n",
      "Epoch 5121 --> loss: 0.10679426701199203\n",
      "Epoch 5131 --> loss: 0.10677251024475518\n",
      "Epoch 5141 --> loss: 0.1067508447515977\n",
      "Epoch 5151 --> loss: 0.10672927000114737\n",
      "Epoch 5161 --> loss: 0.10670778546606662\n",
      "Epoch 5171 --> loss: 0.10668639062301531\n",
      "Epoch 5181 --> loss: 0.10666508495261279\n",
      "Epoch 5191 --> loss: 0.10664386793940099\n",
      "Epoch 5201 --> loss: 0.10662273907180685\n",
      "Epoch 5211 --> loss: 0.10660169784210737\n",
      "Epoch 5221 --> loss: 0.10658074374639216\n",
      "Epoch 5231 --> loss: 0.10655987628452854\n",
      "Epoch 5241 --> loss: 0.10653909496012733\n",
      "Epoch 5251 --> loss: 0.10651839928050585\n",
      "Epoch 5261 --> loss: 0.10649778875665578\n",
      "Epoch 5271 --> loss: 0.10647726290320796\n",
      "Epoch 5281 --> loss: 0.10645682123839877\n",
      "Epoch 5291 --> loss: 0.1064364632840378\n",
      "Epoch 5301 --> loss: 0.10641618856547326\n",
      "Epoch 5311 --> loss: 0.1063959966115621\n",
      "Epoch 5321 --> loss: 0.10637588695463465\n",
      "Epoch 5331 --> loss: 0.10635585913046526\n",
      "Epoch 5341 --> loss: 0.10633591267824014\n",
      "Epoch 5351 --> loss: 0.10631604714052549\n",
      "Epoch 5361 --> loss: 0.10629626206323801\n",
      "Epoch 5371 --> loss: 0.10627655699561352\n",
      "Epoch 5381 --> loss: 0.10625693149017698\n",
      "Epoch 5391 --> loss: 0.10623738510271283\n",
      "Epoch 5401 --> loss: 0.10621791739223596\n",
      "Epoch 5411 --> loss: 0.10619852792096159\n",
      "Epoch 5421 --> loss: 0.10617921625427751\n",
      "Epoch 5431 --> loss: 0.10615998196071486\n",
      "Epoch 5441 --> loss: 0.10614082461191993\n",
      "Epoch 5451 --> loss: 0.10612174378262662\n",
      "Epoch 5461 --> loss: 0.1061027390506289\n",
      "Epoch 5471 --> loss: 0.10608380999675276\n",
      "Epoch 5481 --> loss: 0.10606495620483002\n",
      "Epoch 5491 --> loss: 0.10604617726167104\n",
      "Epoch 5501 --> loss: 0.10602747275703844\n",
      "Epoch 5511 --> loss: 0.10600884228362137\n",
      "Epoch 5521 --> loss: 0.10599028543700859\n",
      "Epoch 5531 --> loss: 0.10597180181566423\n",
      "Epoch 5541 --> loss: 0.10595339102090129\n",
      "Epoch 5551 --> loss: 0.10593505265685667\n",
      "Epoch 5561 --> loss: 0.10591678633046722\n",
      "Epoch 5571 --> loss: 0.10589859165144429\n",
      "Epoch 5581 --> loss: 0.10588046823224966\n",
      "Epoch 5591 --> loss: 0.1058624156880723\n",
      "Epoch 5601 --> loss: 0.10584443363680288\n",
      "Epoch 5611 --> loss: 0.10582652169901212\n",
      "Epoch 5621 --> loss: 0.10580867949792576\n",
      "Epoch 5631 --> loss: 0.10579090665940312\n",
      "Epoch 5641 --> loss: 0.10577320281191328\n",
      "Epoch 5651 --> loss: 0.10575556758651238\n",
      "Epoch 5661 --> loss: 0.10573800061682205\n",
      "Epoch 5671 --> loss: 0.10572050153900656\n",
      "Epoch 5681 --> loss: 0.10570306999175164\n",
      "Epoch 5691 --> loss: 0.10568570561624206\n",
      "Epoch 5701 --> loss: 0.10566840805614047\n",
      "Epoch 5711 --> loss: 0.10565117695756725\n",
      "Epoch 5721 --> loss: 0.10563401196907703\n",
      "Epoch 5731 --> loss: 0.10561691274164052\n",
      "Epoch 5741 --> loss: 0.10559987892862237\n",
      "Epoch 5751 --> loss: 0.1055829101857617\n",
      "Epoch 5761 --> loss: 0.10556600617115074\n",
      "Epoch 5771 --> loss: 0.10554916654521636\n",
      "Epoch 5781 --> loss: 0.10553239097069923\n",
      "Epoch 5791 --> loss: 0.10551567911263438\n",
      "Epoch 5801 --> loss: 0.10549903063833295\n",
      "Epoch 5811 --> loss: 0.10548244521736143\n",
      "Epoch 5821 --> loss: 0.10546592252152391\n",
      "Epoch 5831 --> loss: 0.10544946222484242\n",
      "Epoch 5841 --> loss: 0.10543306400353904\n",
      "Epoch 5851 --> loss: 0.10541672753601743\n",
      "Epoch 5861 --> loss: 0.10540045250284427\n",
      "Epoch 5871 --> loss: 0.10538423858673103\n",
      "Epoch 5881 --> loss: 0.10536808547251676\n",
      "Epoch 5891 --> loss: 0.10535199284715027\n",
      "Epoch 5901 --> loss: 0.10533596039967247\n",
      "Epoch 5911 --> loss: 0.10531998782119852\n",
      "Epoch 5921 --> loss: 0.10530407480490195\n",
      "Epoch 5931 --> loss: 0.10528822104599676\n",
      "Epoch 5941 --> loss: 0.10527242624172005\n",
      "Epoch 5951 --> loss: 0.10525669009131712\n",
      "Epoch 5961 --> loss: 0.10524101229602344\n",
      "Epoch 5971 --> loss: 0.10522539255904877\n",
      "Epoch 5981 --> loss: 0.10520983058556128\n",
      "Epoch 5991 --> loss: 0.10519432608267144\n",
      "Epoch 6001 --> loss: 0.10517887875941552\n",
      "Epoch 6011 --> loss: 0.10516348832674108\n",
      "Epoch 6021 --> loss: 0.10514815449749033\n",
      "Epoch 6031 --> loss: 0.10513287698638563\n",
      "Epoch 6041 --> loss: 0.1051176555100132\n",
      "Epoch 6051 --> loss: 0.10510248978680914\n",
      "Epoch 6061 --> loss: 0.105087379537044\n",
      "Epoch 6071 --> loss: 0.10507232448280739\n",
      "Epoch 6081 --> loss: 0.1050573243479943\n",
      "Epoch 6091 --> loss: 0.10504237885829036\n",
      "Epoch 6101 --> loss: 0.10502748774115678\n",
      "Epoch 6111 --> loss: 0.10501265072581692\n",
      "Epoch 6121 --> loss: 0.10499786754324174\n",
      "Epoch 6131 --> loss: 0.10498313792613613\n",
      "Epoch 6141 --> loss: 0.10496846160892415\n",
      "Epoch 6151 --> loss: 0.10495383832773683\n",
      "Epoch 6161 --> loss: 0.10493926782039807\n",
      "Epoch 6171 --> loss: 0.10492474982641023\n",
      "Epoch 6181 --> loss: 0.10491028408694134\n",
      "Epoch 6191 --> loss: 0.10489587034481311\n",
      "Epoch 6201 --> loss: 0.10488150834448609\n",
      "Epoch 6211 --> loss: 0.10486719783204763\n",
      "Epoch 6221 --> loss: 0.10485293855519913\n",
      "Epoch 6231 --> loss: 0.104838730263243\n",
      "Epoch 6241 --> loss: 0.10482457270706978\n",
      "Epoch 6251 --> loss: 0.1048104656391472\n",
      "Epoch 6261 --> loss: 0.1047964088135057\n",
      "Epoch 6271 --> loss: 0.10478240198572783\n",
      "Epoch 6281 --> loss: 0.10476844491293515\n",
      "Epoch 6291 --> loss: 0.10475453735377678\n",
      "Epoch 6301 --> loss: 0.10474067906841761\n",
      "Epoch 6311 --> loss: 0.10472686981852598\n",
      "Epoch 6321 --> loss: 0.10471310936726239\n",
      "Epoch 6331 --> loss: 0.1046993974792677\n",
      "Epoch 6341 --> loss: 0.10468573392065206\n",
      "Epoch 6351 --> loss: 0.10467211845898351\n",
      "Epoch 6361 --> loss: 0.1046585508632757\n",
      "Epoch 6371 --> loss: 0.1046450309039786\n",
      "Epoch 6381 --> loss: 0.10463155835296571\n",
      "Epoch 6391 --> loss: 0.10461813298352479\n",
      "Epoch 6401 --> loss: 0.10460475457034461\n",
      "Epoch 6411 --> loss: 0.10459142288950721\n",
      "Epoch 6421 --> loss: 0.10457813771847521\n",
      "Epoch 6431 --> loss: 0.10456489883608143\n",
      "Epoch 6441 --> loss: 0.10455170602251952\n",
      "Epoch 6451 --> loss: 0.10453855905933229\n",
      "Epoch 6461 --> loss: 0.1045254577294027\n",
      "Epoch 6471 --> loss: 0.10451240181694248\n",
      "Epoch 6481 --> loss: 0.1044993911074832\n",
      "Epoch 6491 --> loss: 0.10448642538786558\n",
      "Epoch 6501 --> loss: 0.10447350444622967\n",
      "Epoch 6511 --> loss: 0.10446062807200525\n",
      "Epoch 6521 --> loss: 0.10444779605590183\n",
      "Epoch 6531 --> loss: 0.10443500818989969\n",
      "Epoch 6541 --> loss: 0.1044222642672394\n",
      "Epoch 6551 --> loss: 0.10440956408241342\n",
      "Epoch 6561 --> loss: 0.10439690743115573\n",
      "Epoch 6571 --> loss: 0.10438429411043265\n",
      "Epoch 6581 --> loss: 0.10437172391843479\n",
      "Epoch 6591 --> loss: 0.10435919665456636\n",
      "Epoch 6601 --> loss: 0.1043467121194376\n",
      "Epoch 6611 --> loss: 0.10433427011485463\n",
      "Epoch 6621 --> loss: 0.10432187044381079\n",
      "Epoch 6631 --> loss: 0.10430951291047877\n",
      "Epoch 6641 --> loss: 0.10429719732020101\n",
      "Epoch 6651 --> loss: 0.10428492347948096\n",
      "Epoch 6661 --> loss: 0.1042726911959749\n",
      "Epoch 6671 --> loss: 0.10426050027848394\n",
      "Epoch 6681 --> loss: 0.10424835053694431\n",
      "Epoch 6691 --> loss: 0.10423624178242012\n",
      "Epoch 6701 --> loss: 0.10422417382709426\n",
      "Epoch 6711 --> loss: 0.10421214648426158\n",
      "Epoch 6721 --> loss: 0.10420015956831877\n",
      "Epoch 6731 --> loss: 0.10418821289475821\n",
      "Epoch 6741 --> loss: 0.10417630628015866\n",
      "Epoch 6751 --> loss: 0.10416443954217819\n",
      "Epoch 6761 --> loss: 0.10415261249954538\n",
      "Epoch 6771 --> loss: 0.104140824972053\n",
      "Epoch 6781 --> loss: 0.1041290767805484\n",
      "Epoch 6791 --> loss: 0.10411736774692809\n",
      "Epoch 6801 --> loss: 0.10410569769412807\n",
      "Epoch 6811 --> loss: 0.10409406644611771\n",
      "Epoch 6821 --> loss: 0.10408247382789132\n",
      "Epoch 6831 --> loss: 0.104070919665462\n",
      "Epoch 6841 --> loss: 0.10405940378585306\n",
      "Epoch 6851 --> loss: 0.10404792601709151\n",
      "Epoch 6861 --> loss: 0.10403648618820054\n",
      "Epoch 6871 --> loss: 0.10402508412919258\n",
      "Epoch 6881 --> loss: 0.10401371967106207\n",
      "Epoch 6891 --> loss: 0.10400239264577851\n",
      "Epoch 6901 --> loss: 0.10399110288627976\n",
      "Epoch 6911 --> loss: 0.10397985022646468\n",
      "Epoch 6921 --> loss: 0.10396863450118658\n",
      "Epoch 6931 --> loss: 0.10395745554624658\n",
      "Epoch 6941 --> loss: 0.10394631319838635\n",
      "Epoch 6951 --> loss: 0.10393520729528223\n",
      "Epoch 6961 --> loss: 0.10392413767553767\n",
      "Epoch 6971 --> loss: 0.10391310417867786\n",
      "Epoch 6981 --> loss: 0.10390210664514203\n",
      "Epoch 6991 --> loss: 0.10389114491627809\n",
      "Epoch 7001 --> loss: 0.1038802188343352\n",
      "Epoch 7011 --> loss: 0.1038693282424581\n",
      "Epoch 7021 --> loss: 0.10385847298468132\n",
      "Epoch 7031 --> loss: 0.10384765290592148\n",
      "Epoch 7041 --> loss: 0.10383686785197224\n",
      "Epoch 7051 --> loss: 0.10382611766949836\n",
      "Epoch 7061 --> loss: 0.10381540220602854\n",
      "Epoch 7071 --> loss: 0.10380472130995022\n",
      "Epoch 7081 --> loss: 0.10379407483050364\n",
      "Epoch 7091 --> loss: 0.1037834626177757\n",
      "Epoch 7101 --> loss: 0.10377288452269406\n",
      "Epoch 7111 --> loss: 0.10376234039702086\n",
      "Epoch 7121 --> loss: 0.10375183009334833\n",
      "Epoch 7131 --> loss: 0.10374135346509143\n",
      "Epoch 7141 --> loss: 0.10373091036648321\n",
      "Epoch 7151 --> loss: 0.10372050065256902\n",
      "Epoch 7161 --> loss: 0.1037101241792002\n",
      "Epoch 7171 --> loss: 0.10369978080303045\n",
      "Epoch 7181 --> loss: 0.10368947038150728\n",
      "Epoch 7191 --> loss: 0.10367919277286917\n",
      "Epoch 7201 --> loss: 0.10366894783613957\n",
      "Epoch 7211 --> loss: 0.10365873543112057\n",
      "Epoch 7221 --> loss: 0.1036485554183891\n",
      "Epoch 7231 --> loss: 0.10363840765928968\n",
      "Epoch 7241 --> loss: 0.10362829201593154\n",
      "Epoch 7251 --> loss: 0.10361820835118128\n",
      "Epoch 7261 --> loss: 0.10360815652865925\n",
      "Epoch 7271 --> loss: 0.10359813641273367\n",
      "Epoch 7281 --> loss: 0.10358814786851549\n",
      "Epoch 7291 --> loss: 0.10357819076185401\n",
      "Epoch 7301 --> loss: 0.10356826495933147\n",
      "Epoch 7311 --> loss: 0.10355837032825763\n",
      "Epoch 7321 --> loss: 0.10354850673666659\n",
      "Epoch 7331 --> loss: 0.10353867405330966\n",
      "Epoch 7341 --> loss: 0.10352887214765154\n",
      "Epoch 7351 --> loss: 0.10351910088986647\n",
      "Epoch 7361 --> loss: 0.10350936015083165\n",
      "Epoch 7371 --> loss: 0.10349964980212467\n",
      "Epoch 7381 --> loss: 0.10348996971601637\n",
      "Epoch 7391 --> loss: 0.10348031976546827\n",
      "Epoch 7401 --> loss: 0.10347069982412734\n",
      "Epoch 7411 --> loss: 0.10346110976632085\n",
      "Epoch 7421 --> loss: 0.10345154946705225\n",
      "Epoch 7431 --> loss: 0.10344201880199703\n",
      "Epoch 7441 --> loss: 0.10343251764749839\n",
      "Epoch 7451 --> loss: 0.10342304588056125\n",
      "Epoch 7461 --> loss: 0.10341360337885012\n",
      "Epoch 7471 --> loss: 0.10340419002068307\n",
      "Epoch 7481 --> loss: 0.10339480568502792\n",
      "Epoch 7491 --> loss: 0.10338545025149863\n",
      "Epoch 7501 --> loss: 0.10337612360034998\n",
      "Epoch 7511 --> loss: 0.10336682561247394\n",
      "Epoch 7521 --> loss: 0.10335755616939514\n",
      "Epoch 7531 --> loss: 0.1033483151532676\n",
      "Epoch 7541 --> loss: 0.10333910244686952\n",
      "Epoch 7551 --> loss: 0.10332991793359995\n",
      "Epoch 7561 --> loss: 0.10332076149747393\n",
      "Epoch 7571 --> loss: 0.10331163302311984\n",
      "Epoch 7581 --> loss: 0.10330253239577337\n",
      "Epoch 7591 --> loss: 0.1032934595012762\n",
      "Epoch 7601 --> loss: 0.10328441422606942\n",
      "Epoch 7611 --> loss: 0.10327539645719187\n",
      "Epoch 7621 --> loss: 0.10326640608227404\n",
      "Epoch 7631 --> loss: 0.10325744298953692\n",
      "Epoch 7641 --> loss: 0.10324850706778553\n",
      "Epoch 7651 --> loss: 0.10323959820640653\n",
      "Epoch 7661 --> loss: 0.10323071629536519\n",
      "Epoch 7671 --> loss: 0.10322186122519916\n",
      "Epoch 7681 --> loss: 0.10321303288701811\n",
      "Epoch 7691 --> loss: 0.10320423117249725\n",
      "Epoch 7701 --> loss: 0.1031954559738749\n",
      "Epoch 7711 --> loss: 0.10318670718394951\n",
      "Epoch 7721 --> loss: 0.10317798469607421\n",
      "Epoch 7731 --> loss: 0.10316928840415515\n",
      "Epoch 7741 --> loss: 0.10316061820264695\n",
      "Epoch 7751 --> loss: 0.10315197398654985\n",
      "Epoch 7761 --> loss: 0.10314335565140532\n",
      "Epoch 7771 --> loss: 0.1031347630932934\n",
      "Epoch 7781 --> loss: 0.10312619620882918\n",
      "Epoch 7791 --> loss: 0.10311765489515878\n",
      "Epoch 7801 --> loss: 0.10310913904995749\n",
      "Epoch 7811 --> loss: 0.10310064857142416\n",
      "Epoch 7821 --> loss: 0.10309218335827965\n",
      "Epoch 7831 --> loss: 0.10308374330976351\n",
      "Epoch 7841 --> loss: 0.10307532832562952\n",
      "Epoch 7851 --> loss: 0.10306693830614377\n",
      "Epoch 7861 --> loss: 0.10305857315207993\n",
      "Epoch 7871 --> loss: 0.10305023276471785\n",
      "Epoch 7881 --> loss: 0.10304191704583933\n",
      "Epoch 7891 --> loss: 0.10303362589772455\n",
      "Epoch 7901 --> loss: 0.10302535922315047\n",
      "Epoch 7911 --> loss: 0.103017116925386\n",
      "Epoch 7921 --> loss: 0.10300889890819019\n",
      "Epoch 7931 --> loss: 0.10300070507580811\n",
      "Epoch 7941 --> loss: 0.10299253533296938\n",
      "Epoch 7951 --> loss: 0.10298438958488353\n",
      "Epoch 7961 --> loss: 0.10297626773723764\n",
      "Epoch 7971 --> loss: 0.10296816969619381\n",
      "Epoch 7981 --> loss: 0.10296009536838578\n",
      "Epoch 7991 --> loss: 0.10295204466091569\n",
      "Epoch 8001 --> loss: 0.10294401748135219\n",
      "Epoch 8011 --> loss: 0.10293601373772622\n",
      "Epoch 8021 --> loss: 0.10292803333852929\n",
      "Epoch 8031 --> loss: 0.1029200761927103\n",
      "Epoch 8041 --> loss: 0.1029121422096724\n",
      "Epoch 8051 --> loss: 0.10290423129927068\n",
      "Epoch 8061 --> loss: 0.10289634337180863\n",
      "Epoch 8071 --> loss: 0.10288847833803683\n",
      "Epoch 8081 --> loss: 0.10288063610914833\n",
      "Epoch 8091 --> loss: 0.1028728165967781\n",
      "Epoch 8101 --> loss: 0.10286501971299802\n",
      "Epoch 8111 --> loss: 0.10285724537031583\n",
      "Epoch 8121 --> loss: 0.10284949348167192\n",
      "Epoch 8131 --> loss: 0.102841763960437\n",
      "Epoch 8141 --> loss: 0.10283405672040881\n",
      "Epoch 8151 --> loss: 0.1028263716758107\n",
      "Epoch 8161 --> loss: 0.10281870874128718\n",
      "Epoch 8171 --> loss: 0.1028110678319038\n",
      "Epoch 8181 --> loss: 0.10280344886314256\n",
      "Epoch 8191 --> loss: 0.10279585175090024\n",
      "Epoch 8201 --> loss: 0.10278827641148558\n",
      "Epoch 8211 --> loss: 0.10278072276161777\n",
      "Epoch 8221 --> loss: 0.10277319071842227\n",
      "Epoch 8231 --> loss: 0.1027656801994302\n",
      "Epoch 8241 --> loss: 0.10275819112257441\n",
      "Epoch 8251 --> loss: 0.10275072340618809\n",
      "Epoch 8261 --> loss: 0.10274327696900143\n",
      "Epoch 8271 --> loss: 0.10273585173014065\n",
      "Epoch 8281 --> loss: 0.10272844760912367\n",
      "Epoch 8291 --> loss: 0.10272106452585977\n",
      "Epoch 8301 --> loss: 0.10271370240064583\n",
      "Epoch 8311 --> loss: 0.10270636115416497\n",
      "Epoch 8321 --> loss: 0.10269904070748308\n",
      "Epoch 8331 --> loss: 0.10269174098204834\n",
      "Epoch 8341 --> loss: 0.10268446189968693\n",
      "Epoch 8351 --> loss: 0.10267720338260226\n",
      "Epoch 8361 --> loss: 0.10266996535337203\n",
      "Epoch 8371 --> loss: 0.10266274773494612\n",
      "Epoch 8381 --> loss: 0.10265555045064498\n",
      "Epoch 8391 --> loss: 0.10264837342415613\n",
      "Epoch 8401 --> loss: 0.10264121657953357\n",
      "Epoch 8411 --> loss: 0.10263407984119467\n",
      "Epoch 8421 --> loss: 0.10262696313391753\n",
      "Epoch 8431 --> loss: 0.10261986638284046\n",
      "Epoch 8441 --> loss: 0.10261278951345885\n",
      "Epoch 8451 --> loss: 0.10260573245162234\n",
      "Epoch 8461 --> loss: 0.10259869512353471\n",
      "Epoch 8471 --> loss: 0.10259167745574965\n",
      "Epoch 8481 --> loss: 0.10258467937517053\n",
      "Epoch 8491 --> loss: 0.10257770080904707\n",
      "Epoch 8501 --> loss: 0.10257074168497435\n",
      "Epoch 8511 --> loss: 0.10256380193088958\n",
      "Epoch 8521 --> loss: 0.10255688147507118\n",
      "Epoch 8531 --> loss: 0.10254998024613644\n",
      "Epoch 8541 --> loss: 0.10254309817303957\n",
      "Epoch 8551 --> loss: 0.10253623518506982\n",
      "Epoch 8561 --> loss: 0.1025293912118487\n",
      "Epoch 8571 --> loss: 0.10252256618332964\n",
      "Epoch 8581 --> loss: 0.10251576002979501\n",
      "Epoch 8591 --> loss: 0.10250897268185405\n",
      "Epoch 8601 --> loss: 0.10250220407044162\n",
      "Epoch 8611 --> loss: 0.10249545412681624\n",
      "Epoch 8621 --> loss: 0.1024887227825576\n",
      "Epoch 8631 --> loss: 0.10248200996956594\n",
      "Epoch 8641 --> loss: 0.10247531562005822\n",
      "Epoch 8651 --> loss: 0.1024686396665687\n",
      "Epoch 8661 --> loss: 0.10246198204194551\n",
      "Epoch 8671 --> loss: 0.10245534267934878\n",
      "Epoch 8681 --> loss: 0.10244872151225032\n",
      "Epoch 8691 --> loss: 0.10244211847443017\n",
      "Epoch 8701 --> loss: 0.10243553349997597\n",
      "Epoch 8711 --> loss: 0.10242896652328042\n",
      "Epoch 8721 --> loss: 0.10242241747904018\n",
      "Epoch 8731 --> loss: 0.10241588630225389\n",
      "Epoch 8741 --> loss: 0.10240937292822014\n",
      "Epoch 8751 --> loss: 0.10240287729253615\n",
      "Epoch 8761 --> loss: 0.1023963993310965\n",
      "Epoch 8771 --> loss: 0.10238993898009065\n",
      "Epoch 8781 --> loss: 0.10238349617600112\n",
      "Epoch 8791 --> loss: 0.1023770708556031\n",
      "Epoch 8801 --> loss: 0.10237066295596126\n",
      "Epoch 8811 --> loss: 0.10236427241442916\n",
      "Epoch 8821 --> loss: 0.10235789916864761\n",
      "Epoch 8831 --> loss: 0.10235154315654198\n",
      "Epoch 8841 --> loss: 0.10234520431632202\n",
      "Epoch 8851 --> loss: 0.10233888258647969\n",
      "Epoch 8861 --> loss: 0.10233257790578697\n",
      "Epoch 8871 --> loss: 0.10232629021329509\n",
      "Epoch 8881 --> loss: 0.10232001944833283\n",
      "Epoch 8891 --> loss: 0.1023137655505047\n",
      "Epoch 8901 --> loss: 0.1023075284596895\n",
      "Epoch 8911 --> loss: 0.10230130811603921\n",
      "Epoch 8921 --> loss: 0.10229510445997655\n",
      "Epoch 8931 --> loss: 0.10228891743219458\n",
      "Epoch 8941 --> loss: 0.10228274697365408\n",
      "Epoch 8951 --> loss: 0.10227659302558284\n",
      "Epoch 8961 --> loss: 0.10227045552947436\n",
      "Epoch 8971 --> loss: 0.10226433442708552\n",
      "Epoch 8981 --> loss: 0.10225822966043521\n",
      "Epoch 8991 --> loss: 0.10225214117180483\n",
      "Epoch 9001 --> loss: 0.10224606890373311\n",
      "Epoch 9011 --> loss: 0.10224001279901856\n",
      "Epoch 9021 --> loss: 0.10223397280071542\n",
      "Epoch 9031 --> loss: 0.10222794885213357\n",
      "Epoch 9041 --> loss: 0.10222194089683656\n",
      "Epoch 9051 --> loss: 0.10221594887864029\n",
      "Epoch 9061 --> loss: 0.10220997274161189\n",
      "Epoch 9071 --> loss: 0.10220401243006803\n",
      "Epoch 9081 --> loss: 0.102198067888574\n",
      "Epoch 9091 --> loss: 0.10219213906194144\n",
      "Epoch 9101 --> loss: 0.10218622589522788\n",
      "Epoch 9111 --> loss: 0.10218032833373528\n",
      "Epoch 9121 --> loss: 0.10217444632300834\n",
      "Epoch 9131 --> loss: 0.10216857980883352\n",
      "Epoch 9141 --> loss: 0.1021627287372374\n",
      "Epoch 9151 --> loss: 0.10215689305448535\n",
      "Epoch 9161 --> loss: 0.1021510727070809\n",
      "Epoch 9171 --> loss: 0.10214526764176368\n",
      "Epoch 9181 --> loss: 0.10213947780550851\n",
      "Epoch 9191 --> loss: 0.10213370314552393\n",
      "Epoch 9201 --> loss: 0.10212794360925141\n",
      "Epoch 9211 --> loss: 0.102122199144363\n",
      "Epoch 9221 --> loss: 0.10211646969876162\n",
      "Epoch 9231 --> loss: 0.10211075522057884\n",
      "Epoch 9241 --> loss: 0.10210505565817346\n",
      "Epoch 9251 --> loss: 0.10209937096013103\n",
      "Epoch 9261 --> loss: 0.10209370107526197\n",
      "Epoch 9271 --> loss: 0.10208804595260097\n",
      "Epoch 9281 --> loss: 0.10208240554140519\n",
      "Epoch 9291 --> loss: 0.10207677979115339\n",
      "Epoch 9301 --> loss: 0.10207116865154503\n",
      "Epoch 9311 --> loss: 0.1020655720724983\n",
      "Epoch 9321 --> loss: 0.10205999000414974\n",
      "Epoch 9331 --> loss: 0.10205442239685257\n",
      "Epoch 9341 --> loss: 0.10204886920117566\n",
      "Epoch 9351 --> loss: 0.10204333036790265\n",
      "Epoch 9361 --> loss: 0.10203780584803077\n",
      "Epoch 9371 --> loss: 0.10203229559276872\n",
      "Epoch 9381 --> loss: 0.1020267995535371\n",
      "Epoch 9391 --> loss: 0.10202131768196658\n",
      "Epoch 9401 --> loss: 0.10201584992989621\n",
      "Epoch 9411 --> loss: 0.1020103962493732\n",
      "Epoch 9421 --> loss: 0.10200495659265152\n",
      "Epoch 9431 --> loss: 0.1019995309121906\n",
      "Epoch 9441 --> loss: 0.1019941191606543\n",
      "Epoch 9451 --> loss: 0.10198872129091074\n",
      "Epoch 9461 --> loss: 0.10198333725602934\n",
      "Epoch 9471 --> loss: 0.10197796700928123\n",
      "Epoch 9481 --> loss: 0.10197261050413807\n",
      "Epoch 9491 --> loss: 0.10196726769427078\n",
      "Epoch 9501 --> loss: 0.10196193853354796\n",
      "Epoch 9511 --> loss: 0.10195662297603575\n",
      "Epoch 9521 --> loss: 0.10195132097599635\n",
      "Epoch 9531 --> loss: 0.1019460324878869\n",
      "Epoch 9541 --> loss: 0.10194075746635849\n",
      "Epoch 9551 --> loss: 0.1019354958662559\n",
      "Epoch 9561 --> loss: 0.10193024764261516\n",
      "Epoch 9571 --> loss: 0.10192501275066354\n",
      "Epoch 9581 --> loss: 0.10191979114581871\n",
      "Epoch 9591 --> loss: 0.1019145827836873\n",
      "Epoch 9601 --> loss: 0.10190938762006349\n",
      "Epoch 9611 --> loss: 0.10190420561092955\n",
      "Epoch 9621 --> loss: 0.10189903671245276\n",
      "Epoch 9631 --> loss: 0.10189388088098651\n",
      "Epoch 9641 --> loss: 0.10188873807306753\n",
      "Epoch 9651 --> loss: 0.10188360824541703\n",
      "Epoch 9661 --> loss: 0.10187849135493697\n",
      "Epoch 9671 --> loss: 0.1018733873587119\n",
      "Epoch 9681 --> loss: 0.10186829621400673\n",
      "Epoch 9691 --> loss: 0.10186321787826516\n",
      "Epoch 9701 --> loss: 0.10185815230910975\n",
      "Epoch 9711 --> loss: 0.10185309946434147\n",
      "Epoch 9721 --> loss: 0.10184805930193684\n",
      "Epoch 9731 --> loss: 0.10184303178004923\n",
      "Epoch 9741 --> loss: 0.10183801685700647\n",
      "Epoch 9751 --> loss: 0.10183301449131056\n",
      "Epoch 9761 --> loss: 0.10182802464163651\n",
      "Epoch 9771 --> loss: 0.101823047266832\n",
      "Epoch 9781 --> loss: 0.10181808232591594\n",
      "Epoch 9791 --> loss: 0.10181312977807756\n",
      "Epoch 9801 --> loss: 0.10180818958267583\n",
      "Epoch 9811 --> loss: 0.10180326169923884\n",
      "Epoch 9821 --> loss: 0.10179834608746245\n",
      "Epoch 9831 --> loss: 0.10179344270720936\n",
      "Epoch 9841 --> loss: 0.10178855151850871\n",
      "Epoch 9851 --> loss: 0.10178367248155529\n",
      "Epoch 9861 --> loss: 0.10177880555670789\n",
      "Epoch 9871 --> loss: 0.10177395070448973\n",
      "Epoch 9881 --> loss: 0.10176910788558605\n",
      "Epoch 9891 --> loss: 0.10176427706084508\n",
      "Epoch 9901 --> loss: 0.10175945819127583\n",
      "Epoch 9911 --> loss: 0.10175465123804801\n",
      "Epoch 9921 --> loss: 0.1017498561624904\n",
      "Epoch 9931 --> loss: 0.1017450729260913\n",
      "Epoch 9941 --> loss: 0.10174030149049673\n",
      "Epoch 9951 --> loss: 0.10173554181751022\n",
      "Epoch 9961 --> loss: 0.10173079386909174\n",
      "Epoch 9971 --> loss: 0.10172605760735622\n",
      "Epoch 9981 --> loss: 0.10172133299457439\n",
      "Epoch 9991 --> loss: 0.10171661999317057\n",
      "Best estimate for \"beta\": [10.29912949  1.12892015 -0.1995038 ]\n"
     ]
    }
   ],
   "source": [
    "beta = np.zeros(Z.shape[1])\n",
    "\n",
    "print(f'Starting with \"beta\": {beta}')\n",
    "\n",
    "epochs: int = 10000\n",
    "learning_rate: float = 0.0001\n",
    "\n",
    "for epoch in pbar(range(epochs)):\n",
    "    # Calculate the \"predictions\" (squishified dot product of `beta` and `x`) based on our current `beta` vector\n",
    "    ys_pred = np.array([dot_sigmoid(beta, x) for x in Z])\n",
    "    # Calculate and print the error\n",
    "    if epoch % 10 == True:\n",
    "        loss: float = error(Y, ys_pred)\n",
    "        print(f'Epoch {epoch} --> loss: {loss}')\n",
    "        if(loss < 0.08):\n",
    "            break\n",
    "        \n",
    "\n",
    "    # Calculate the gradient\n",
    "    grad = [0. for _ in range(len(beta))]\n",
    "    for x, y in zip(Z, Y):\n",
    "        err: float = dot_sigmoid(beta, x) - y\n",
    "        for i, x_i in enumerate(x):\n",
    "            grad[i] += (err * x_i)\n",
    "    grad = [1 / len(x) * g_i for g_i in grad]\n",
    "\n",
    "    # Take a small step in the direction of greatest decrease\n",
    "    beta = np.array([b + (gb * -learning_rate) for b, gb in zip(beta, grad)]).ravel()\n",
    "    # print(f'Epoch {epoch} beta: {beta}')\n",
    "\n",
    "print(f'Best estimate for \"beta\": {beta}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.0  %\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(len(X_dev)):\n",
    "    zprime = [x for x in X_dev[i]]\n",
    "    zprime.insert(0,1)\n",
    "    pred_val = dot_sigmoid(beta, zprime)\n",
    "    if(pred_val >= 0.5):\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "    if(pred == int(Y_dev[i])):\n",
    "        acc += 1\n",
    "print(acc*100/len(X_dev), \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(theta, X):\n",
    "    return 1 / (1 + np.exp(-(np.dot(theta, X.T)))) - 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y, theta):\n",
    "    y1 = hypothesis(X, theta)\n",
    "    return -(1/len(X)) * np.sum(y*np.log(y1) + (1-y)*np.log(1-y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, epochs):\n",
    "    m = len(X)\n",
    "    for i in range(0, epochs):\n",
    "        for j in range(0, 2):\n",
    "            theta = pd.DataFrame(theta)\n",
    "            h = hypothesis(theta.iloc[:,j], X)\n",
    "            for k in range(0, theta.shape[0]):\n",
    "                theta.iloc[k, j] -= (alpha/m) * np.sum((h-y.iloc[:, j])*X.iloc[:, k])\n",
    "            theta = pd.DataFrame(theta)\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 2)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Files\\PRML\\Assignment 4\\test_logit_syn.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Files/PRML/Assignment%204/test_logit_syn.ipynb#ch0000007?line=0'>1</a>\u001b[0m theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros([df_X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], encoder_df\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Files/PRML/Assignment%204/test_logit_syn.ipynb#ch0000007?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(theta\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Files/PRML/Assignment%204/test_logit_syn.ipynb#ch0000007?line=2'>3</a>\u001b[0m theta \u001b[39m=\u001b[39m gradient_descent(df_X, encoder_df, theta, \u001b[39m0.02\u001b[39;49m, \u001b[39m1500\u001b[39;49m)\n",
      "\u001b[1;32md:\\Files\\PRML\\Assignment 4\\test_logit_syn.ipynb Cell 7'\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(X, y, theta, alpha, epochs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Files/PRML/Assignment%204/test_logit_syn.ipynb#ch0000006?line=5'>6</a>\u001b[0m         h \u001b[39m=\u001b[39m hypothesis(theta\u001b[39m.\u001b[39miloc[:,j], X[j])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Files/PRML/Assignment%204/test_logit_syn.ipynb#ch0000006?line=6'>7</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, theta\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Files/PRML/Assignment%204/test_logit_syn.ipynb#ch0000006?line=7'>8</a>\u001b[0m             theta\u001b[39m.\u001b[39miloc[k, j] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m (alpha\u001b[39m/\u001b[39mm) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum((h\u001b[39m-\u001b[39my\u001b[39m.\u001b[39miloc[:, j])\u001b[39m*\u001b[39mX\u001b[39m.\u001b[39;49miloc[:, k])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Files/PRML/Assignment%204/test_logit_syn.ipynb#ch0000006?line=8'>9</a>\u001b[0m         theta \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(theta)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/PRML/Assignment%204/test_logit_syn.ipynb#ch0000006?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m theta, cost\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:961\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=958'>959</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=959'>960</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=960'>961</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=961'>962</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=962'>963</a>\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=963'>964</a>\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:1458\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1455'>1456</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getitem_tuple\u001b[39m(\u001b[39mself\u001b[39m, tup: \u001b[39mtuple\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1457'>1458</a>\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_tuple_indexer(tup)\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1458'>1459</a>\u001b[0m     \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1459'>1460</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_lowerdim(tup)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:769\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=766'>767</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, k \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(key):\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=767'>768</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=768'>769</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_key(k, i)\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=769'>770</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=770'>771</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=771'>772</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLocation based indexing can only have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=772'>773</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_types\u001b[39m}\u001b[39;00m\u001b[39m] types\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=773'>774</a>\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:1361\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_key\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1358'>1359</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1359'>1360</a>\u001b[0m \u001b[39melif\u001b[39;00m is_integer(key):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1360'>1361</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_integer(key, axis)\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1361'>1362</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1362'>1363</a>\u001b[0m     \u001b[39m# a tuple should already have been caught by this point\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1363'>1364</a>\u001b[0m     \u001b[39m# so don't treat a tuple as a valid indexer\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1364'>1365</a>\u001b[0m     \u001b[39mraise\u001b[39;00m IndexingError(\u001b[39m\"\u001b[39m\u001b[39mToo many indexers\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:1452\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1449'>1450</a>\u001b[0m len_axis \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1450'>1451</a>\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m len_axis \u001b[39mor\u001b[39;00m key \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mlen_axis:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/pandas/core/indexing.py?line=1451'>1452</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msingle positional indexer is out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "theta = np.zeros([df_X.shape[0], encoder_df.shape[1]])\n",
    "print(theta.shape)\n",
    "theta = gradient_descent(df_X, encoder_df, theta, 0.02, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8976"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRG = linear_model.LogisticRegression(\n",
    "   random_state = 100,solver = 'liblinear',multi_class = 'auto'\n",
    ").fit(X, Y.ravel())\n",
    "LRG.score(X, Y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRG.score(X_dev, Y_dev.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94ae5e5b5ae5a1ee57a607c9497711b6bf7cc4fc8b73b07c0408939ea9c64789"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
